{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will apply pca to breast cancer dataset having 30 features.\n",
    "2. We will reduce the components to 15 (half of 30)\n",
    "3. We will see how PCA affects our accuracy and time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from sklearn import preprocessing,datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = datasets.load_breast_cancer()\n",
    "x = b.data\n",
    "y = b.target\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = preprocessing.StandardScaler()\n",
    "\n",
    "x1 = sc.fit_transform(x) # fit the data as well as tranform it into feature scaled one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train,y_test = train_test_split(x1,y,random_state = 2,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without PCA results :\n",
      "Time_Taken : 0.0069768428802490234\n",
      "Score : 0.972027972028\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "start = time.time()\n",
    "clf.fit(x_train,y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Without PCA results :\")\n",
    "print(\"Time_Taken :\" ,end-start)\n",
    "print(\"Score :\" , clf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 15)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With PCA results :\n",
      "Time_Taken : 0.003724336624145508\n",
      "Score : 0.916083916084\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf.fit(x_train_pca,y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"With PCA results :\")\n",
    "print(\"Time_Taken :\" ,end-start)\n",
    "print(\"Score :\" , clf.score(x_test_pca,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It can be seen as accuracy drops a little bit, But time is reduced too when PCA is being used.\n",
    "\n",
    "$ Also the dataset is smaller (only 550 enteries) , so when the dataset is huge , PCA will surely reduce time taken a lot. Improving the speed by a considerable amount without loosing much accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
